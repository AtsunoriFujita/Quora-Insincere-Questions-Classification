{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:13:24.582045Z",
     "start_time": "2018-12-16T14:13:24.575596Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:00.493036Z",
     "start_time": "2018-12-16T14:05:00.488893Z"
    },
    "_uuid": "4b31c825c98e542a4f344e884c7e181563282ba8"
   },
   "outputs": [],
   "source": [
    "notebookstart= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:01.159045Z",
     "start_time": "2018-12-16T14:05:01.151109Z"
    },
    "_uuid": "f0dbed43f8e3843751a70bcb094e0585888d9150"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 65 # max number of words in a question to use\n",
    "\n",
    "n_splits = 4\n",
    "batch_size = 2048\n",
    "train_epochs = 5\n",
    "LR = 0.001\n",
    "WD = 0.0001\n",
    "\n",
    "SEED = 916 #916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:01.788449Z",
     "start_time": "2018-12-16T14:05:01.785817Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=916):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3fa86551eff75df96ba71c33dfce7d382710d28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores: 2\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_partitions = 20  # number of partitions to split dataframe\n",
    "num_cores = psutil.cpu_count()  # number of cores on your machine\n",
    "\n",
    "print('number of cores:', num_cores)\n",
    "\n",
    "def df_parallelize_run(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:05.593474Z",
     "start_time": "2018-12-16T14:05:05.558263Z"
    },
    "_uuid": "5013e3945f2454854337ff3ee7eee98efa79fd25"
   },
   "outputs": [],
   "source": [
    "def clean_text(x, maxlen=None):\n",
    "    puncts = ['ь','ɾ','█','℅ι','_','#','ò','ᡤ','▲','ุ','\\\\','ύ','═','―','ʍ','∧','β','‑','²','▬','®','●','φ','б',\n",
    "          '‡','ϕ','„','ấ','ʻ','＝','¯','£','⁴','˂','∖','ñ','ɛ','ø','č','и','చ','＾','ô','－','ʋ','Ü','ⁿ','∆','±',\n",
    "          'Ó','Â','ế','™','’','⊕','Ż','π','Ľ','ψ','△','ʌ','ě','♭','$','ộ','ᠰ','ε','ʏ','à','օ','ú','п','ö','া',\n",
    "          'о','ệ','(','»','ü',')','∑','ን','Γ','￼','ĺ','ా','Ō','ç','Ο','⅔','∪','ɨ','♫','ʖ','⟨','∫','ϵ','：','、',\n",
    "          '╩','´','ỵ','℅','∠','∩','%','ː','с','→','ạ','͡','¬','ķ','з','@','ה','℃','í','ட','☹','Č','\"','ῆ','Я','⃗',\n",
    "          '”','╚','▒','ד','←','Ã','✌','़','П','Å','ן','λ','ā','§','ἰ','ֿ','↑','ì','∂','…','.','“','ɒ','›','≥','−',\n",
    "          'ㅜ','ል','≤','¼','¶','æ','ß','∨','χ','║','ū','⧼','°','ả','†','Š','ə','，','¤','ు','г','ሮ','ō','★','ɑ',\n",
    "          '「','~','ł','...', 'ి','ý','•','⊆','÷','ч',';','Í','Ø','،','ο','[','≠','י','=','≅','ụ','Σ','╗','ù','!',\n",
    "          'â','︡','ె','¹','é','ν','ኤ','∞','⋅','♥','︠','}','ɦ','ﬁ','·','?','Ñ','▀','✅','ễ','ợ',']','✓','ת','▾','▓',\n",
    "          ':','ᡠ','│','ή','የ','（','ş','³','>','è','ξ','¦','ğ','л','ᠨ','¨','ὤ','ف','ኢ','Ž','∘','ς','₹','͜','‹','α',\n",
    "          'న','ᡳ','ɖ','╣','ʀ','ו','■','×','☺','►','ኝ','º','አ','⧽','ɔ','＞','ᠠ','▄','⊨','╔','√','å','ć','¿','ీ','Φ',\n",
    "          'İ','░','兰','ζ','š','В','క','・','{','）','ã','½','Δ','©','ن','<','τ','⁷','^',\"'\",'‘','ś','σ','ḵ','¢',\n",
    "          '′','ద','х','ం','⌚','ర','/','ք','ũ','∈','ê','ሁ','Ź','—','á','̃','*','θ','–','î','ι','⁰','ṭ','ɸ','ä','¸',\n",
    "          'ռ','É','ḥ','♪','ï','`','д','∀','ʿ','❤','|','ő','☆','ș','О','€','¥','+','ó','▪','⟩','⇒','≈','─','▼','ž',\n",
    "          'Ἀ','я','Ā','↓','ℇ','ᡵ','-','µ','«','δ','μ','ῥ','ʃ','♨','″','&','Ω','♦','ʊ','గ',',','¾','∙','η','ī','ċ',\n",
    "          '╦','ẽ','⁴λ']\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in puncts[:maxlen]:\n",
    "        if punct in x:  # add this line\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\n",
    "                'wasnt':'was not','Doesnt':'Does not','Couldn':'Could not','Whatare':'What are','Howdo':'How do',\n",
    "                'Didnt':'Did not','Howmany':'How many','Howcan':'How can','Isnt':'Is not',\n",
    "                'Shouldnt':'Should not','howto':'how to','Cannot':'Can not','doI':'do I',\n",
    "                \"whatis\":\"what is\",'Whatis':'What is','hasnt':'has not','practise':'practice','behaviours':'behaviors',\n",
    "                'colour':'color',\n",
    "                'centre':'center','favourite':'favorite','favour':'favor','travelling':'traveling','counselling':'counseling',\n",
    "                'theatre':'theater','cancelled':'canceled','labour':'labor','organisation':'organization','organised':'organized',\n",
    "                'wwii':'world war 2','citicise':'criticize','youtu ':'youtube ','qoura':'Quora','sallary':'salary',\n",
    "                'whta': 'what','Whta':'What','narcisist': 'narcissist','howdo':'how do','whatare':'what are','howcan':'how can',\n",
    "                'howmuch':'how much','Howmuch':'How much','howmany':'how many','whydo':'why do','Whydo':'Why do',\n",
    "                'doI': 'do I','theBest':'the best','socialising':'socializing','visualise':'visualize',\n",
    "                'howdoes':'how does','mastrubation':'masturbation','mastrubate':'masturbate','masterbation':'masturbation',\n",
    "                'masterbate':'masturbate',\"mastrubating\":'masturbating','masterbating':'masturbating',\n",
    "                'labelled':'labeled','civilisation':'civilization','customised':'customized','polarisation':'polarization',\n",
    "                'pennis':'penis','etherium':'ethereum','narcissit': 'narcissist','bigdata':'big data',\n",
    "                #'2k17':'2017','2k18':'2018',\n",
    "                'qouta':'quota','exboyfriend':'ex boyfriend','airhostess':'air hostess',\"whst\":'what',\"Whst\":'what',\n",
    "                'watsapp': 'WhatsApp','demonitisation':'demonetization','demonitization':'demonetization',\n",
    "                'didnt':'did not','doesnt':'does not','isnt':'is not','shouldnt':'should not',\n",
    "                'aeroplane':'airplane','aeroplanes':'airplanes','rumours':'rumors','armour':'armor','odour':'odor',\n",
    "                \"tryin'\":\"trying\",'quorans':'Quora users','Quorans':'Quora users',\n",
    "                \"quoran\":\"Quora user\",'recognise':'recognize',\n",
    "                'cryptocurrencies':'crypto currency','cryptocurrency':'crypto currency','aluminium':'aluminum',\n",
    "                'friendzoned':'friend zoned','legalised':'legalized','intership':'internship',\n",
    "                \"brexit\":'leave EU',\"Brexit\":'leave EU',\"blockchain\":\"Blockchain\",'licence':'license','cheque':'check',\n",
    "                'practising':'practicing','wwwyoutubecom':'youtube','worshipping':'worshiping','apologise':'apologize',\n",
    "                'neighbouring':'neighboring','jewellery':'jewelry','neighbour':'neighbor','behavioural':'behavioral',\n",
    "                'neighbourhood':'neighborhood','counselling':'counseling','h1b':'H1B','civilised':'civilized',\n",
    "                'blockchains':'Blockchain', 'demonetisation':'demonetization','bitcoins':'Bitcoin','ethereum':'Ethereum',\n",
    "                'Bitcoins':'Bitcoin','criticised':'criticized','rumour':'rumor','organising':'organizing',\n",
    "                'spoilt':'spoiled','flavoured':'flavored','dysfuntional':'dysfunctional','schizoids':'schizoid',\n",
    "                'wierdest':'weirdest','intercaste':'inter_caste','JCPOA':'Joint Comprehensive Plan of Action',\n",
    "                'bhakts':'Bhakti','Bhakts':'Bhakti','honours':'honors','learnt':'learned','selfie':'self snap','selfies':'self snap',\n",
    "                'organise':'organize','neurotypicals':'normal','criticise':'criticize',\n",
    "                'trumpism':'Trump principle','Trumpism':'Trump principle','tamilans':'Tamilians','acturial':'actuarial',\n",
    "                'judgement':'judgment','licences':'licenses','legalise':'legalize','undergraduation':'undergraduate',\n",
    "                'centralised':'centralized','biharis':'Biharis','tumour':'tumor','labelling':'labeling',\n",
    "                'whyis':'why is','airpods':'AirPods','zhihu':'Zhihua','globalisation':'globalization',\n",
    "                'sjws':'SJW', 'neuralink':'neurolink','fullform':'full-form',\n",
    "                'cisgender':'normal gender','friendzone':'friend zone','colonisation':'colonization','nationalised':'nationalized',\n",
    "                'xiomi':'Xiaomi', 'rohingya':'Rohingya','despacito':'desposito',\n",
    "                'fortnite':'Fortnite','bittrex':'Bittrex',\n",
    "                'reactjs':'javascript','nodejs':'javascript','programr':'programer',\n",
    "                'hyperloop':'Hyperloop','aadhaar':'Aadhaar','baahubali':'Bahubali','Baahubali':'Bahubali',\n",
    "                'snapchat':'Instagram','Snapchat':'Instagram','SnapChat':'Instagram','realise':'realize',\n",
    "                'defence':'defense','offence':'offense',\n",
    "                'btech':'bachelor_degree','Btech':'bachelor_degree','BTECH':'bachelor_degree','mtech':'master_degree',\n",
    "                'behaviour':'behavior','anaesthesia':'Anaesthesia','incels':'involuntary celibate',\n",
    "                \"‘\":\"'\", \"´\":\"'\", \"—\":\"-\", \"₹\":\"rupee\",\n",
    "                \"–\":\"-\", \"’\":\"'\", \"_\":\"-\", \"`\":\"'\", '“':'\"', '”':'\"', '“':'\"', \n",
    "                '•':'.', '−':'-', \n",
    "                'rahul':'Rahul','upvotes':'up vote', 'upvote':'up vote','upvoted':'up voted', 'downvote':'down vote',\n",
    "                'downvotes':'down votes','downvoted':'down voted','mhtcet':'MHT_CET','MHTCET':'MHT_CET',\n",
    "                'Qoura':'Quora','Quoras':'Quora','Quara':'Quora','∞':'infinity',\n",
    "                'wwwquoracom':'Quora','nootropics':'smart drugs','Nootropics':'smart drugs',\n",
    "                '\\ ufeff':\" \", 'करना':\"\",\n",
    "                '∅':'phi','Doklam':'standoff',\n",
    "                'donald':'Donald','trump':'Trump','TRUMP':'Trump','drumpf':'serial liar','Drumpf':'serial liar',\n",
    "                'hillary':'Hillary', 'clinton':'Clinton','Trumpcare':'Trump plan','Trumpers':'Trump supporter',\n",
    "                'narendra':'Narendra','gandhi':'Gandhi',\n",
    "                'barack':'Barack','obama':'Obama','hussein':'Hussein',\n",
    "                'obamacare':'Obama plan','Obamacare':'Obama plan','aadhar':'Aadhaar',\n",
    "                'AlShamsi':'Al_Shamsi','auschwitz':'Auschwitz','muhammed':'Muhammed','aAadhaar':'Aadhaar',\n",
    "                'whatsapp':'WhatsApp', 'instagram':'Instagram','sjw':'SJW','aadhaar':'Aadhaar','AAadhaar':'Aadhaar',\n",
    "                'bahubali':'Bahubali', 'bitcoin':'Bitcoin', 'bhakti':'Bhakti', 'javascript':'JavaScript','kotlin':'Java',\n",
    "                'madheshi':'Madheshi', 'quora':'Quora','SJWs':'SJW','femdom':'female dominance',\n",
    "                'crossdress':'transvestite','paedophile':'pedophile'\n",
    "               }\n",
    "\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b2bc684e19c1af0b3683cda66d967487c796ee67"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    preprocess text main steps\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = replace_typical_misspell(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_clean_wrapper(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(preprocess)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:05.960523Z",
     "start_time": "2018-12-16T14:05:05.954488Z"
    },
    "_uuid": "87a5fb67cdd061388568a1cd27250cecf5fb24a7"
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].apply(len)\n",
    "    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    train_df = df_parallelize_run(train_df, text_clean_wrapper)\n",
    "    test_df = df_parallelize_run(test_df, text_clean_wrapper)\n",
    "    \n",
    "    ###################### Add Features ###############################\n",
    "    #  https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb\n",
    "    train = add_features(train_df)\n",
    "    test = add_features(test_df)\n",
    "\n",
    "    features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "    test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "    ###########################################################################\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=False, filters=[])\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, tokenizer.word_index, features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:06.244573Z",
     "start_time": "2018-12-16T14:05:06.228668Z"
    },
    "_uuid": "9fa4b3016d2edf2c14340de1f23988b086850851"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index, max_words=max_features, embed_size=300):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    emb_mean, emb_std = -0.005838493338505765, 0.48782081729236354 #-0.005838499, 0.48782197 \n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words, embed_size))\n",
    "    with open(EMBEDDING_FILE, encoding=\"utf8\") as f: #, 'r'\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_words:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '))[:300] #, dtype='float32'\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    print('mean: ', emb_mean, 'std: ', emb_std)\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word.lower())\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_w2v(word_index):\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    print('vocab:',len(word2vec.vocab))\n",
    "    all_embs = word2vec.vectors\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    print(emb_mean,emb_std)\n",
    "    print(max_features,' from ',len(word_index.items()))\n",
    "    # num_words = min(num_words, len(tokenizer.word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    \n",
    "    # embedding_matrix = np.zeros((num_words, dim))\n",
    "    count = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i>=max_features:\n",
    "            break\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            count += 1\n",
    "    del word2vec\n",
    "    print('embedding matrix size:',embedding_matrix.shape)\n",
    "    print('Number of words not in vocab:',count)\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_fasttext(word_index, max_words=max_features, embed_size=300):\n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    emb_mean, emb_std = -0.0033469954096391496, 0.10985541316945975 #-0.0033469985, 0.109855495\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words, embed_size))\n",
    "    with open(EMBEDDING_FILE, encoding=\"utf8\") as f: #, 'r'\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_words:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '))[:300] #, dtype='float32'\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "732aa8b2a17d9ac5d175d69f7b5e356962849137"
   },
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
    "\n",
    "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
    "    their `License\n",
    "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (:class:`torch.FloatTensor` [batch size, sequence length, rnn hidden size]): Input to\n",
    "                apply dropout too.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
    "        mask = mask.div_(1 - self.p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8bf124b07facff07aa52f4a035507e879a0acc77"
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    ''' Simple Linear layer with xavier init '''\n",
    "\n",
    "    def __init__(self, d_in, d_out, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(d_in, d_out, bias=bias)\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2a73abd77e1af56cd696ab5b9d2a88a2b638392a"
   },
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, batch_size=None, output_size=1, hidden_size=100, vocab_size=max_features, embedding_length=embed_size):\n",
    "        super(RCNN, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embedding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), sparse=False)\n",
    "        self.lockeddropout = LockedDropout(p=0.2)\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size, bidirectional=True, batch_first=False)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=False)\n",
    "        self.input = nn.Linear(embedding_length, 150)\n",
    "        self.W_s1 = nn.Linear(hidden_size*2+150, 150, bias=True)\n",
    "        self.W_s2 = nn.Linear(150, 65, bias=True)\n",
    "        self.lrelu = torch.nn.LeakyReLU()\n",
    "        self.W2 = Linear(2*hidden_size+150, 100)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.label = Linear(102, output_size)\n",
    "        \n",
    "        \n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
    "        encoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
    "        the input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
    "        connected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
    "        pos & neg.\n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output))) #/ self.temper\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "    \n",
    "\n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        The idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n",
    "        of the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of \n",
    "        its own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n",
    "        state. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n",
    "        vector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n",
    "        dimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n",
    "        \"\"\"\n",
    "        input = self.word_embeddings(input_sentence[0]) # embedded input of shape = (batch_size, num_sequences, embedding_length)\n",
    "        input = self.lockeddropout(input) \n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.empty(2, len(input[1]), self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
    "            h_0 = nn.init.xavier_uniform_(h_0, gain=5/3)\n",
    "            c_0 = Variable(torch.empty(2, len(input[1]), self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
    "            c_0 = nn.init.xavier_uniform_(c_0, gain=5/3)\n",
    "        else:\n",
    "            h_0 = Variable(torch.empty(2, batch_size, self.hidden_size).cuda())\n",
    "            h_0 = nn.init.xavier_uniform_(h_0, gain=5/3)\n",
    "            c_0 = Variable(torch.empty(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = nn.init.xavier_uniform_(c_0, gain=5/3)\n",
    "\n",
    "        output, (final_hidden_state,_) = self.lstm(input, (h_0, c_0))\n",
    "        output, _ = self.gru(output, final_hidden_state)\n",
    "        \n",
    "        input = torch.tanh(self.input(input))\n",
    "        final_encoding = torch.cat((output, input), 2) \n",
    "        final_encoding = final_encoding.permute(1, 0, 2) \n",
    "        attn_weight_matrix = self.attention_net(final_encoding) \n",
    "        final_encoding = torch.matmul(attn_weight_matrix, final_encoding)\n",
    "        final_encoding = self.lrelu(final_encoding)        \n",
    "        y = self.W2(final_encoding)\n",
    "        y = y.permute(0, 2, 1)\n",
    "        y = self.maxpool(y)\n",
    "        f = torch.tensor(input_sentence[1], dtype=torch.float).cuda(non_blocking=True)\n",
    "        y = y.squeeze(2)\n",
    "        y = torch.cat((y, f),1)\n",
    "        y = self.dropout_2(y)\n",
    "        logits = self.label(y)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:05:07.844744Z",
     "start_time": "2018-12-16T14:05:07.806098Z"
    },
    "_uuid": "49db4897c91305a4381e64e8eb94b93aa7d5a98e"
   },
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                # p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                p.data.add_(-step_size,  torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom) )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "61c7f459afe10956b96afe85fcbc6090ae966408"
   },
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:09:30.194824Z",
     "start_time": "2018-12-16T14:05:08.734989Z"
    },
    "_uuid": "61243e77515cc0c44d2bb84fa9ea115483e21a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -0.0053247833 std:  0.49346462\n",
      "vocab: 3000000\n",
      "-0.003527845 0.13315111\n",
      "120000  from  230120\n",
      "embedding matrix size: (120000, 300)\n",
      "Number of words not in vocab: 21494\n",
      "Took 7.82 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_X, test_X, train_y, word_index, features, test_features = load_and_prec()\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_para(word_index)\n",
    "embedding_matrix_3 = load_w2v(word_index)\n",
    "embedding_matrix_4 = load_fasttext(word_index)\n",
    "\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "85bf5b282186f07b58aac1f03278ded29c9c7895"
   },
   "outputs": [],
   "source": [
    "del word_index, mispell_dict, mispellings, mispellings_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "87807dc5f4e3e9bb60409af3bb3313e8b9147076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 1200)\n",
      "(120000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1)\n",
    "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4 \n",
    "print(np.shape(embedding_matrix))\n",
    "\n",
    "pca = PCA(n_components=300, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=SEED)\n",
    "pca.fit(embedding_matrix)\n",
    "embedding_matrix = pca.transform(embedding_matrix)\n",
    "\n",
    "print(np.shape(embedding_matrix))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:09:30.416779Z",
     "start_time": "2018-12-16T14:09:30.196308Z"
    },
    "_uuid": "4bc17062b48b74557b5e4d7b94014dd3a5f87e48"
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=916).split(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:09:30.419437Z",
     "start_time": "2018-12-16T14:09:30.417619Z"
    },
    "_uuid": "b6c179e5478f1ac92b69020fb875bf3d8154bff3"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "88c988fe31c9a75726bcb97c8d6faa66c353addd"
   },
   "outputs": [],
   "source": [
    "def z_score(array, threshold):\n",
    "    x = array - threshold\n",
    "    tmp = np.sum(x ** 2)\n",
    "    tmp = np.sqrt(tmp / len(array))\n",
    "    x = x / tmp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:09:30.452768Z",
     "start_time": "2018-12-16T14:09:30.420716Z"
    },
    "_uuid": "2ad4bf11f982e199ff5d64004eeedd9f3d3d454c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, classification_report\n",
    "\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:10:51.689454Z",
     "start_time": "2018-12-16T14:10:51.677826Z"
    },
    "_uuid": "69b8d2e9046f1b772a32f3d106e971a34999555d"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "        \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "843158c9f701dd369775abcc06ee5917f208ead5"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "18a0805d0eabf5da19b62d8ff64d77278d812e3a"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, pos_weight=None, reduction='elementwise_mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=self.pos_weight)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none', pos_weight=self.pos_weight)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction is None:\n",
    "            return F_loss\n",
    "        else:\n",
    "            return torch.mean(F_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T15:25:05.722819Z",
     "start_time": "2018-12-16T15:09:04.858524Z"
    },
    "_uuid": "1f80aa8c099cb76eede9c0c5db859a835fc9d1f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 \t loss=0.0651 \t val_loss=0.0552 \t val_f1=0.6615 best_t=0.44 \t time=298.69s\n",
      "Best:  0.6615169505122973\n",
      "Epoch 2/5 \t loss=0.0534 \t val_loss=0.0520 \t val_f1=0.6802 best_t=0.44 \t time=298.85s\n",
      "Best:  0.6801935454420961\n",
      "Epoch 3/5 \t loss=0.0496 \t val_loss=0.0513 \t val_f1=0.6867 best_t=0.50 \t time=298.66s\n",
      "Best:  0.6866682017038914\n",
      "Epoch 4/5 \t loss=0.0472 \t val_loss=0.0505 \t val_f1=0.6913 best_t=0.49 \t time=298.76s\n",
      "Best:  0.6912983906551172\n",
      "Epoch 5/5 \t loss=0.0447 \t val_loss=0.0511 \t val_f1=0.6952 best_t=0.53 \t time=298.67s\n",
      "Best:  0.6951837185744545\n",
      "model loaded from model_best.pth.tar\n",
      "{'threshold': 0.5285874605178833, 'f1': 0.695209677792427}\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=0.0654 \t val_loss=0.0541 \t val_f1=0.6668 best_t=0.43 \t time=298.59s\n",
      "Best:  0.6668197685100128\n",
      "Epoch 2/5 \t loss=0.0535 \t val_loss=0.0518 \t val_f1=0.6788 best_t=0.47 \t time=298.75s\n",
      "Best:  0.6787645495371644\n",
      "Epoch 3/5 \t loss=0.0498 \t val_loss=0.0508 \t val_f1=0.6848 best_t=0.50 \t time=298.76s\n",
      "Best:  0.6848091549787174\n",
      "Epoch 4/5 \t loss=0.0474 \t val_loss=0.0499 \t val_f1=0.6932 best_t=0.50 \t time=298.58s\n",
      "Best:  0.6932269045662637\n",
      "Epoch 5/5 \t loss=0.0450 \t val_loss=0.0492 \t val_f1=0.6943 best_t=0.49 \t time=298.40s\n",
      "Best:  0.6943422913719943\n",
      "model loaded from model_best.pth.tar\n",
      "{'threshold': 0.4883240759372711, 'f1': 0.6942858491011182}\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=0.0657 \t val_loss=0.0540 \t val_f1=0.6691 best_t=0.46 \t time=298.50s\n",
      "Best:  0.669125542742425\n",
      "Epoch 2/5 \t loss=0.0536 \t val_loss=0.0517 \t val_f1=0.6834 best_t=0.49 \t time=298.87s\n",
      "Best:  0.6834337490075195\n",
      "Epoch 3/5 \t loss=0.0498 \t val_loss=0.0502 \t val_f1=0.6896 best_t=0.47 \t time=299.09s\n",
      "Best:  0.6896160736558042\n",
      "Epoch 4/5 \t loss=0.0474 \t val_loss=0.0496 \t val_f1=0.6953 best_t=0.47 \t time=298.87s\n",
      "Best:  0.6953116000460776\n",
      "Epoch 5/5 \t loss=0.0449 \t val_loss=0.0500 \t val_f1=0.6964 best_t=0.44 \t time=298.67s\n",
      "Best:  0.6963525905026281\n",
      "model loaded from model_best.pth.tar\n",
      "{'threshold': 0.44153979420661926, 'f1': 0.6963369797104081}\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=0.0662 \t val_loss=0.0551 \t val_f1=0.6608 best_t=0.40 \t time=298.79s\n",
      "Best:  0.6607713760968642\n",
      "Epoch 2/5 \t loss=0.0538 \t val_loss=0.0516 \t val_f1=0.6768 best_t=0.48 \t time=296.98s\n",
      "Best:  0.6768164144844708\n",
      "Epoch 3/5 \t loss=0.0501 \t val_loss=0.0514 \t val_f1=0.6858 best_t=0.52 \t time=293.00s\n",
      "Best:  0.685813549507817\n",
      "Epoch 4/5 \t loss=0.0477 \t val_loss=0.0495 \t val_f1=0.6935 best_t=0.45 \t time=292.98s\n",
      "Best:  0.6934867663981588\n",
      "Epoch 5/5 \t loss=0.0451 \t val_loss=0.0494 \t val_f1=0.6946 best_t=0.50 \t time=293.04s\n",
      "Best:  0.6945751679895109\n",
      "model loaded from model_best.pth.tar\n",
      "{'threshold': 0.49877694249153137, 'f1': 0.6945485876627157}\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros((len(train_X)))\n",
    "train_preds2 = np.zeros((len(train_X)))\n",
    "test_preds = np.zeros((len(test_X)))\n",
    "\n",
    "seed_torch(SEED)\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    \n",
    "    best_prec1 = 0\n",
    "    \n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long)#.cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32)#.cuda()\n",
    "    kfold_X_features = torch.tensor(features[train_idx.astype(int)], dtype=torch.long)#.cuda()\n",
    "    kfold_X_valid_features = torch.tensor(features[valid_idx.astype(int)], dtype=torch.long)#.cuda()\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long)#.cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32)#.cuda()\n",
    "\n",
    "    model = RCNN()\n",
    "    model.cuda()\n",
    "    \n",
    "    class_weight = torch.FloatTensor([1.25]).cuda()    \n",
    "    gamma = 1.3\n",
    "    loss_fn = FocalLoss(gamma=gamma, pos_weight=class_weight)\n",
    "    \n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.001, 0.003   \n",
    "    optimizer = AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.98), weight_decay=0)\n",
    "    ################################################################################################\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range', gamma=0.99994)\n",
    "    ###############################################################################################\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                                              pin_memory=True, num_workers=4)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False,\n",
    "                                              pin_memory=True, num_workers=4)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch.cuda(non_blocking=True), f])\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch.cuda(non_blocking=True))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        valid_true_fold = np.zeros((x_val_fold.size(0)))\n",
    "\n",
    "        valid_preds_tmp = np.zeros((x_val_fold.size(0)))\n",
    "        valid_true_tmp = np.zeros((x_val_fold.size(0)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "            y_pred = model([x_batch.cuda(non_blocking=True),f]).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch.cuda(non_blocking=True)).item() / len(valid_loader)\n",
    "            valid_preds_tmp[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            valid_true_tmp[i * batch_size:(i+1) * batch_size] = y_batch.cpu().numpy()[:, 0]\n",
    "\n",
    "        search_result = threshold_search(valid_true_tmp, valid_preds_tmp)\n",
    "        val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        \n",
    "        prec1 = val_f1\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print('Best: ', best_prec1)\n",
    "    \n",
    "    del x_train_fold, y_train_fold, train_loader\n",
    "    \n",
    "    checkpoint_path = 'model_best.pth.tar'\n",
    "    model = RCNN()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), weight_decay=0)\n",
    "    load_checkpoint(checkpoint_path, model, optimizer)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "        f = kfold_X_valid_features[index]\n",
    "        y_pred = model([x_batch.cuda(non_blocking=True),f]).detach()\n",
    "\n",
    "        valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        valid_true_fold[i * batch_size:(i+1) * batch_size] = y_batch.cpu().numpy()[:, 0]\n",
    "\n",
    "    search_result = threshold_search(valid_true_fold, valid_preds_fold)\n",
    "    print(search_result)\n",
    "    train_preds2[valid_idx] = z_score(valid_preds_fold, search_result['threshold'])\n",
    "    \n",
    "    del x_val_fold, y_val_fold, valid_loader\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds_fold = z_score(test_preds_fold, search_result['threshold'])\n",
    "    test_preds += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "ed3f4fa72d887aa9814aaa3872e1abe14db49376"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.4996938705444336, 'f1': 0.6929971922982039}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "b68a87d01b4949d3ab2167799c8769513c1397bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.0, 'f1': 0.6951061266104366}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds2)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "c3581f74ae694eb07182e2a23c9db8f01a78f1ba"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub.prediction = test_preds > 0\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "22e3654348c2345f541e86899dfc22172dc7a572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Runtime: 110.63 Minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "381fef40c02ac313759eda569394d7413f35e7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid,prediction\r\n",
      "00014894849d00ba98a9,False\r\n",
      "000156468431f09b3cae,False\r\n",
      "000227734433360e1aae,False\r\n",
      "0005e06fbe3045bd2a92,False\r\n",
      "00068a0f7f41f50fc399,False\r\n",
      "000a2d30e3ffd70c070d,False\r\n",
      "000b67672ec9622ff761,False\r\n",
      "000b7fb1146d712c1105,False\r\n",
      "000d665a8ddc426a1907,False\r\n"
     ]
    }
   ],
   "source": [
    "!rm model_best.pth.tar\n",
    "!rm checkpoint.pth.tar\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "8433a11cb6784ea5fb17692c11f43ccb1cc2cceb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
